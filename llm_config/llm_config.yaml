# LLM Configuration
# Supports multiple providers: OpenAI, Anthropic (Claude), Google (Gemini), DeepSeek, etc.

# Active LLM provider (one of: openai, anthropic, google, deepseek, azure_openai)
active_provider: openai

# Provider-specific configurations
providers:
  # OpenAI Configuration
  openai:
    enabled: true
    api_key: ${OPENAI_API_KEY}  # From environment variable
    model: gpt-4
    temperature: 0.3
    max_tokens: 2000
    timeout: 60
    max_retries: 3
    base_url: null  # Optional: for custom endpoints
    organization: null  # Optional: OpenAI organization ID
    
  # Anthropic (Claude) Configuration
  anthropic:
    enabled: false
    api_key: ${ANTHROPIC_API_KEY}  # From environment variable
    model: claude-3-opus-20240229
    temperature: 0.3
    max_tokens: 2000
    timeout: 60
    max_retries: 3
    
  # Google (Gemini) Configuration
  google:
    enabled: false
    api_key: ${GOOGLE_API_KEY}  # From environment variable
    model: gemini-pro
    temperature: 0.3
    max_tokens: 2000
    timeout: 60
    max_retries: 3
    location: us-central1  # Optional: for Vertex AI
    
  # DeepSeek Configuration
  deepseek:
    enabled: false
    api_key: ${DEEPSEEK_API_KEY}  # From environment variable
    model: deepseek-chat
    temperature: 0.3
    max_tokens: 2000
    timeout: 60
    max_retries: 3
    base_url: https://api.deepseek.com
    
  # Azure OpenAI Configuration
  azure_openai:
    enabled: false
    api_key: ${AZURE_OPENAI_API_KEY}  # From environment variable
    model: gpt-4
    temperature: 0.3
    max_tokens: 2000
    timeout: 60
    max_retries: 3
    azure_endpoint: ${AZURE_OPENAI_ENDPOINT}
    api_version: 2024-02-15-preview
    deployment_name: gpt-4  # Azure deployment name

# Default settings applied to all providers
defaults:
  temperature: 0.3
  max_tokens: 2000
  timeout: 60
  max_retries: 3

# Model-specific overrides (optional)
model_overrides:
  gpt-4:
    temperature: 0.2
    max_tokens: 4000
  claude-3-opus-20240229:
    temperature: 0.3
    max_tokens: 4000
  gemini-pro:
    temperature: 0.4
    max_tokens: 3000

